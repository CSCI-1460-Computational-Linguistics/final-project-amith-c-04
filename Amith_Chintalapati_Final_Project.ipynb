{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPK_c2EALwij"
      },
      "source": [
        "# Semantic Parsing Final Project\n",
        "Link to the paper: https://aclanthology.org/P16-1004.pdf\n",
        "\n",
        "Read through the paper fully before starting the assignment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "b0MLqDYLdLHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf810750-10bd-4d36-f0a0-647fb622de02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FILEPATH = \"drive/MyDrive/data/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mewu8d2qACH"
      },
      "source": [
        "# Data Downloading\n",
        "This cell obtains the pre-processed Jobs dataset (see the paper) that you will be using to train and evaluate your model. (Pre-processed meaning that argument identification, section 3.6, has already been done for you). You should only need to run this cell ***once***. Feel free to delete it after running. Create a folder in your Google Drive in which the code below will store the pre-processed data needed for this project. Modify `FILEPATH` above to direct to said folder. It should start with `drive/MyDrive/...`, feel free to take a look at previous assignments that use mounting Google Drive if you can't remember what it should look like. *Make sure the data path ends with a slash character ('/').* The below code will access the zip file containing the pre-processed Jobs dataset from the paper and extract the files into your folder! Feel free to take a look at the `train.txt` and `test.txt` files to see what the data looks like. :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hfJFfYRSFBV"
      },
      "source": [
        "# Data Pre-processing\n",
        "The following code is defined for you! It extracts the queries (inputs to your Seq2Seq model) and logical forms (expected outputs) from the training and testing files. It also does important pre-processing such as padding the queries and logical forms and turns the words into vocab indices. **Look over and understand this code before you start the assignment!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "oEwaCwJhb9kL"
      },
      "outputs": [],
      "source": [
        "def extract_file(filename):\n",
        "  \"\"\"\n",
        "  Extracts queries and corresponding logical forms from either\n",
        "  train.txt or test.txt. (Feel free to take a look at the files themselves\n",
        "  in your Drive!)\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  filename : str\n",
        "      name of the file to extract from\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[list[list[str]], list[list[str]]]\n",
        "      a tuple of a list of queries and their corresponding logical forms\n",
        "      each in the form of a list of string tokens\n",
        "  \"\"\"\n",
        "  queries, logical_forms = [], []\n",
        "  with open(FILEPATH + filename) as f:\n",
        "    for line in f:\n",
        "      line = line.strip() # remove new line character\n",
        "      query, logical_form = line.split('\\t')\n",
        "\n",
        "      query = query.split(' ')[::-1] # reversed inputs are used the paper (section 4.2)\n",
        "      logical_form = [\"<s>\"] + logical_form.split(' ') + [\"</s>\"]\n",
        "\n",
        "      queries.append(query)\n",
        "      logical_forms.append(logical_form)\n",
        "  return queries, logical_forms\n",
        "\n",
        "query_train, lf_train = extract_file('train.txt') # 500 instances\n",
        "query_test, lf_test = extract_file('test.txt') # 140 instances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KEG4r-BpA3mH"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "query_vocab = Counter()\n",
        "for l in query_train:\n",
        "  query_vocab.update(l)\n",
        "\n",
        "query_word2idx = {}\n",
        "for w, c in query_vocab.items():\n",
        "  if c >= 2:\n",
        "    query_word2idx[w] = len(query_word2idx)\n",
        "query_word2idx['<UNK>'] = len(query_word2idx)\n",
        "query_word2idx['<PAD>'] = len(query_word2idx)\n",
        "query_idx2word = {i:word for word,i in query_word2idx.items()}\n",
        "\n",
        "query_vocab = list(query_word2idx.keys())\n",
        "\n",
        "lf_vocab = Counter()\n",
        "for lf in lf_train:\n",
        "  lf_vocab.update(lf)\n",
        "\n",
        "lf_vocab['<UNK>'] = 0\n",
        "lf_vocab['<PAD>'] = 0\n",
        "lf_idx2word = {i:word for i, word in enumerate(lf_vocab.keys())}\n",
        "lf_word2idx = {word:i for i, word in lf_idx2word.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6NH1EXAqDgnR"
      },
      "outputs": [],
      "source": [
        "query_train_tokens = [[query_word2idx.get(w, query_word2idx['<UNK>']) for w in l] for l in query_train]\n",
        "query_test_tokens = [[query_word2idx.get(w, query_word2idx['<UNK>']) for w in l] for l in query_test]\n",
        "\n",
        "lf_train_tokens = [[lf_word2idx.get(w, lf_word2idx['<UNK>']) for w in l] for l in lf_train]\n",
        "lf_test_tokens = [[lf_word2idx.get(w, lf_word2idx['<UNK>']) for w in l] for l in lf_test]\n",
        "\n",
        "def pad(seq, max_len, pad_token_idx):\n",
        "  \"\"\"\n",
        "  Pads a given sequence to the max length using the given padding token index\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  seq : list[int]\n",
        "      sequence in the form of a list of vocab indices\n",
        "  max_len : int\n",
        "      length sequence should be padded to\n",
        "  pad_token_idx\n",
        "      vocabulary index of the padding token\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  list[int]\n",
        "      padded sequence\n",
        "  \"\"\"\n",
        "  seq = seq[:max_len]\n",
        "  padded_seq = seq + (max_len - len(seq)) * [pad_token_idx]\n",
        "  return padded_seq\n",
        "\n",
        "query_max_target_len = max([len(i) for i in query_train_tokens])\n",
        "query_train_tokens = [pad(i, query_max_target_len, query_word2idx['<PAD>']) for i in query_train_tokens]\n",
        "query_test_tokens = [pad(i, query_max_target_len, query_word2idx['<PAD>']) for i in query_test_tokens]\n",
        "\n",
        "lf_max_target_len = int(max([len(i) for i in lf_train_tokens]) * 1.5)\n",
        "lf_train_tokens = [pad(i, lf_max_target_len, lf_word2idx['<PAD>']) for i in lf_train_tokens]\n",
        "lf_test_tokens = [pad(i, lf_max_target_len, lf_word2idx['<PAD>']) for i in lf_test_tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCKjb4HsMKw-"
      },
      "source": [
        "# Data Loading\n",
        "The following code creates a JobsDataset and DataLoaders to use with your implemented model. Take a look at the main function at the end of this stencil to see how they are used in context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PginNNZ2sqqN"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, default_collate\n",
        "\n",
        "class JobsDataset(Dataset):\n",
        "  \"\"\"Defines a Dataset object for the Jobs dataset to be used with Dataloader\"\"\"\n",
        "  def __init__(self, queries, logical_forms):\n",
        "    \"\"\"\n",
        "    Initializes a JobsDataset\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    queries : list[list[int]]\n",
        "        a list of queries, which have been tokenized and padded, in the form\n",
        "        of a list of vocab indices\n",
        "    logical_forms : list[list[int]]\n",
        "        a list of corresponding logical forms, which have been tokenized and\n",
        "        padded, in the form of a list of vocab indices\n",
        "    \"\"\"\n",
        "    self.queries = queries\n",
        "    self.logical_forms = logical_forms\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    \"\"\"\n",
        "    Returns the amount of paired queries and logical forms in the dataset\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    int\n",
        "        length of the dataset\n",
        "    \"\"\"\n",
        "    return len(self.queries)\n",
        "\n",
        "  def __getitem__(self, idx: int) -> tuple[list[int], list[int]]:\n",
        "    \"\"\"\n",
        "    Returns a paired query and logical form at the specified index\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    idx : int\n",
        "        specified index of the dataset\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    tuple[list[int], list[int]]\n",
        "        paired query and logical form at the specified index, in the form of\n",
        "        a list of vocab indices\n",
        "    \"\"\"\n",
        "    return self.queries[idx], self.logical_forms[idx]\n",
        "\n",
        "def build_datasets() -> tuple[JobsDataset, JobsDataset]:\n",
        "  \"\"\"\n",
        "  Builds a train and a test dataset from the queries and logical forms\n",
        "  train and test tokens\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[JobsDataset, JobsDataset]\n",
        "      a training and testing JobsDataset\n",
        "  \"\"\"\n",
        "  jobs_train = JobsDataset(queries=query_train_tokens, logical_forms=lf_train_tokens)\n",
        "  jobs_test = JobsDataset(queries=query_test_tokens, logical_forms=lf_test_tokens)\n",
        "  return jobs_train, jobs_test\n",
        "\n",
        "def collate(batch : list[tuple[list[int], list[int]]]) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "  \"\"\"\n",
        "  Used as collate_fn when creating the Dataloaders from the dataset\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  batch : list[tuple[list[int], list[int]]]\n",
        "      a list of outputs of __getitem__\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[torch.Tensor, torch.Tensor]\n",
        "      a batched set of input sequences and a batched set of target sequences\n",
        "  \"\"\"\n",
        "  src, tgt = default_collate(batch)\n",
        "  return torch.stack(src), torch.stack(tgt)\n",
        "\n",
        "def build_dataloaders(dataset_train: JobsDataset, dataset_test: JobsDataset,\n",
        "                      train_batch_size: int) -> tuple[DataLoader, DataLoader]:\n",
        "  \"\"\"\n",
        "  Used as collate_fn when creating the Dataloaders from the dataset, batching\n",
        "  the training data according to the inputted batch size and batching the\n",
        "  testing data with a batch size of 1\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  dataset_train : JobsDataset\n",
        "      training dataset\n",
        "  dataset_test : JobsDataset\n",
        "      testing dataset\n",
        "  train_batch_size : int\n",
        "      batch size to be used during training\n",
        "\n",
        "  Returns\n",
        "  ----------\n",
        "  tuple[DataLoader, DataLoader]\n",
        "      a training and testing DataLoader\n",
        "  \"\"\"\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=train_batch_size, shuffle=True, collate_fn=collate)\n",
        "  dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate)\n",
        "  return dataloader_train, dataloader_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCDXsRIBIC42"
      },
      "source": [
        "# TODO: Define your model here!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY_VOCAB_LEN = len(query_vocab)\n",
        "LF_VOCAB_LEN = len(lf_vocab)"
      ],
      "metadata": {
        "id": "h_BsmJf3sbDc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, hidden_size: int, embed_size: int, layers: int, drop_rate: float):\n",
        "        \"\"\"\n",
        "        Initializes an Encoder module\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        vocab_size : int\n",
        "            size of the encoder input vocabulary\n",
        "        hidden_size : int\n",
        "            size of the hidden states for the LSTM\n",
        "        embed_size : int\n",
        "            size of the embeddings for each word\n",
        "        layers : int\n",
        "            number of layers of the LSTM\n",
        "        drop_rate : float\n",
        "            rate at which weights are probabalistically excluded from the model\n",
        "        \"\"\"\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)\n",
        "        self.lstm_layers = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=layers, dropout=drop_rate, batch_first=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Feeds a tensor through the Encoder\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : int\n",
        "            input tensor for the Encoder\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
        "            the output, hidden_state, and cell state that is outputted from the LSTM layers\n",
        "        \"\"\"\n",
        "        emb = self.embedding_layer(x) # (B x L x E)\n",
        "        output, (hidden_state, cell_state) = self.lstm_layers(emb) # Output: (B x L x H), HS: (L x B x H)\n",
        "        return output, hidden_state, cell_state"
      ],
      "metadata": {
        "id": "aQ5vsS3FwGU1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, n: int):\n",
        "        \"\"\"\n",
        "        Initializes an Attention module\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n : int\n",
        "            dimensions of the Linear Layers used in the attention calculations (should be the size of hidden_state for matrix mult. to work)\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.W1 = nn.Linear(n, n) # dimensions: (hidden_size, hidden_size) bc matrix mult with h\n",
        "        self.W2 = nn.Linear(n, n) # dimensions: (hidden_size, hidden_size) bc matrix mult with c\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, enc_outputs: torch.Tensor, dec_hidden_state: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Feeds Encoder outputs and a Decoder hidden state through the Encoder\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        enc_outputs : torch.Tensor\n",
        "            outputs of the encoder which show the end states at every time step in the LSTM\n",
        "        dec_hidden_state : torch.Tensor\n",
        "            current hidden state from the decoder\n",
        "        Returns\n",
        "        ----------\n",
        "        torch.Tensor\n",
        "            the new hidden state after attention is taken in from the encoder\n",
        "        \"\"\"\n",
        "        # *Print statements for video*\n",
        "        #print(\"--------------------------------\")\n",
        "        # enc_outputs: 21 x 20 x 256 B x S x H\n",
        "        # print(\"Starting Encoder Out Dim\", enc_outputs.size())\n",
        "        # print(\"Starting Hidden State Dim\", dec_hidden_state.size())\n",
        "\n",
        "        dec_hidden_state = dec_hidden_state.permute(1, 2, 0) # B x H x L\n",
        "        dec_hidden_state = dec_hidden_state[:,:,-1].unsqueeze(2) # B x H x 1\n",
        "\n",
        "        # print(\"Second Hidden State Dim\", dec_hidden_state.size())\n",
        "\n",
        "        att_scores = torch.bmm(enc_outputs, dec_hidden_state) # B x S x 1\n",
        "        att_scores = self.softmax(att_scores) # B x S x 1\n",
        "\n",
        "        # print(\"Attention Scores Dim\", att_scores.size())\n",
        "\n",
        "        enc_outputs  = enc_outputs.permute(0, 2, 1) # B x H x S\n",
        "\n",
        "        # print(\"Second Encoder Out Dim\", enc_outputs.size())\n",
        "\n",
        "        context_vects = torch.bmm(enc_outputs, att_scores).squeeze(2) # B x H x 1 -> B x H\n",
        "\n",
        "        # print(\"Context Vector Dim\", context_vects.size())\n",
        "\n",
        "        dec_hidden_state = dec_hidden_state.squeeze(2) # B x H x 1 -> B x H\n",
        "\n",
        "        # print(\"Third Hidden State Dim\", dec_hidden_state.size())\n",
        "\n",
        "        cur_dec = self.W1(dec_hidden_state)\n",
        "        cur_enc = self.W2(context_vects)\n",
        "\n",
        "        # print(\"Post Lin Layer 1 Dim\", cur_dec.size())\n",
        "        # print(\"Post Lin Layer 2 Dim\", cur_enc.size())\n",
        "\n",
        "        # print(\"--------------------------------\")\n",
        "        return self.tanh(cur_dec + cur_enc)"
      ],
      "metadata": {
        "id": "3tk_imboACqk"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting Encoder Out Dim: torch.Size([21, 20, 256])\n",
        "\n",
        "Starting Hidden State Dim: torch.Size([1, 21, 256])\n",
        "\n",
        "Second Hidden State Dim: torch.Size([21, 256, 1])\n",
        "\n",
        "Attention Scores Dim: torch.Size([21, 20, 1])\n",
        "\n",
        "Second Encoder Out Dim: torch.Size([21, 256, 20])\n",
        "\n",
        "Context Vector Dim: torch.Size([21, 256])\n",
        "\n",
        "Third Hidden State Dim: torch.Size([21, 256])\n",
        "\n",
        "Post Lin Layer 1 Dim: torch.Size([21, 256])\n",
        "\n",
        "Post Lin Layer 2 Dim: torch.Size([21, 256])"
      ],
      "metadata": {
        "id": "lfxeHC2de_uO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, hidden_size: int, embed_size: int, layers: int, drop_rate: float, attention: Attention):\n",
        "        \"\"\"\n",
        "        Initializes a Decoder module\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        vocab_size : int\n",
        "            size of the decoder output\n",
        "        hidden_size : int\n",
        "            size of the hidden states for the LSTM\n",
        "        embed_size : int\n",
        "            size of the embeddings for each word\n",
        "        layers : int\n",
        "            number of layers of the LSTM\n",
        "        drop_rate : float\n",
        "            rate at which weights are probabalistically excluded from the model\n",
        "        attention : Attention\n",
        "            the Attention obj that the Decoder will use on its hidden states\n",
        "        \"\"\"\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)\n",
        "        self.lstm_layers = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=layers, dropout=drop_rate, batch_first=True)\n",
        "        self.attention = attention\n",
        "        self.W0 = nn.Linear(hidden_size, vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, lf_input: torch.Tensor, enc_outputs: torch.Tensor, prev_hs: torch.Tensor, prev_cs: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Feeds an input sequence, encoder outputs, a previous hidden state, and a previous cell state through the Decoder\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        lf_input : torch.Tensor\n",
        "            the input sequence that the Decoder builds off of\n",
        "        enc_outputs : torch.Tensor\n",
        "            outputs of the encoder which show the end states at every time step in the LSTM\n",
        "        prev_hs : torch.Tensor\n",
        "            the previous Decoder hidden state (at the start this will be the Encoder's outputted hidden state)\n",
        "        prev_cs : torch.Tensor\n",
        "            the previous Decoder cell state (at the start this will be the Encoder's outputted cell state)\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
        "            the output of the decoder that gives probabilities for lf predictions at each time stamp, the final hidden state of the Decoder, and the final cell state of the Decoder\n",
        "        \"\"\"\n",
        "        emb = self.embedding_layer(lf_input) # Input: list of id's of lf's | Output: Matrix of embs (len(vocab) x emb_size)\n",
        "        emb = emb.unsqueeze(1)  # Ouptut: Tensor of embs that can be correctly passed into lstm (S x 1 x E)\n",
        "        output, (hs, cs) = self.lstm_layers(emb, (prev_hs, prev_cs))  # output: (1 x 1 x E), hs: ()\n",
        "        h_att = self.attention(enc_outputs, hs)\n",
        "        prob_dists = self.softmax(self.W0(h_att))\n",
        "        return prob_dists, hs, cs"
      ],
      "metadata": {
        "id": "wqGOortdz9zY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncDec(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, device: str):\n",
        "        \"\"\"\n",
        "        Initializes an Encoder-Decoder module\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        encoder : Encoder\n",
        "            Encoder module\n",
        "        decoder : Decoder\n",
        "            Decoder module\n",
        "        device : str\n",
        "            device that the model is running on\n",
        "        \"\"\"\n",
        "        super(EncDec, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, input: torch.Tensor, tgt: torch.Tensor, teacher_force_prob=1): # src: (batch_size x list of vocab_ids), tgt: (batch_size x tgt_len x emb_size)\n",
        "        \"\"\"\n",
        "        Feeds an input tensor and tgt tensor through the Encoder-Decoder\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input : torch.Tensor\n",
        "            the input sequence that gets fed through the model\n",
        "        tgt : torch.Tensor\n",
        "            the target sequence that gets fed through the model\n",
        "        teacher_forcing_ratio : float\n",
        "            the probability of a model output being used in the sequence (defaulted at 1)\n",
        "\n",
        "        Returns\n",
        "        ----------\n",
        "        torch.Tensor\n",
        "            the output tensor of the Encoder-Decoder model after running through both the Encoder and Decoder\n",
        "        \"\"\"\n",
        "        enc_outputs, enc_hidden_state, enc_cell_state = self.encoder(input)\n",
        "        tgt_vocab_size = self.decoder.embedding_layer.num_embeddings\n",
        "        outputs = torch.zeros(tgt.shape[0], tgt.shape[1], tgt_vocab_size).to(self.device)\n",
        "\n",
        "        lf_input = tgt[:, 0]\n",
        "        hs = enc_hidden_state\n",
        "        cs = enc_cell_state\n",
        "        for t in range(1, tgt.shape[1]):\n",
        "            probs, hs, cs = self.decoder(lf_input, enc_outputs, hs, cs)\n",
        "            outputs[:, t, :] = probs\n",
        "            if torch.rand(1) < teacher_force_prob:\n",
        "                lf_input = tgt[:, t]\n",
        "            else:\n",
        "                lf_input = torch.argmax(probs, dim=1)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "F8Yh6ya4KzoT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "NG376y1VUkOh"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    \"\"\"\n",
        "    Returns\n",
        "    ----------\n",
        "    Model\n",
        "        Encoder-Decoder model!\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    hidden_dim = 256\n",
        "    emb_dim = 128\n",
        "    layers = 1\n",
        "    drop_rate = 0.02\n",
        "\n",
        "    encoder = Encoder(QUERY_VOCAB_LEN, hidden_dim, emb_dim, layers, drop_rate)\n",
        "    attention = Attention(hidden_dim)\n",
        "    decoder = Decoder(LF_VOCAB_LEN, hidden_dim, emb_dim, layers, drop_rate, attention)\n",
        "    model = EncDec(encoder, decoder, device).to(device)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YiYNa1FINe6"
      },
      "source": [
        "# TODO: Training and testing loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2OdOyg8RHrc1"
      },
      "outputs": [],
      "source": [
        "LF_SOS_INDEX = lf_word2idx['<s>']\n",
        "LF_EOS_INDEX = lf_word2idx['</s>']\n",
        "LF_PAD_INDEX = lf_word2idx['<PAD>']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model: nn.Module,\n",
        "                train_dataloader: DataLoader,\n",
        "                loss_fn: nn.Module,\n",
        "                optimizer: torch.optim.Optimizer,\n",
        "                device: str) -> float:\n",
        "    \"\"\"\n",
        "    Trains the inputted model using the provided data, optimizer, loss\n",
        "    function, and device for one epoch. Returns the average loss of the epoch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        model to train\n",
        "    train_dataloader : DataLoader\n",
        "        training data\n",
        "    loss_fn : nn.Module\n",
        "        loss function to use with training\n",
        "    optimizer : torch.optim.Optimizer\n",
        "        optimizer to use with training\n",
        "    device : str\n",
        "        device that the model is running on\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    float\n",
        "        epoch average loss\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for input, tgt in train_dataloader:\n",
        "        input, tgt = input.to(device).permute(1, 0), tgt.to(device).permute(1, 0)\n",
        "        outputs = model(input, tgt)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = outputs[:, 1:, :]\n",
        "        tgt = tgt[:, 1:]\n",
        "        batch_size, seq_len, vocab_size = outputs.size()\n",
        "        flattened_outputs = outputs.reshape(batch_size * seq_len, vocab_size)\n",
        "        flattened_tgt = tgt.reshape(batch_size * seq_len)\n",
        "\n",
        "        loss = loss_fn(flattened_outputs, flattened_tgt)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return total_loss / len(train_dataloader)\n",
        "\n",
        "def train(model: nn.Module,\n",
        "          train_dataloader : DataLoader,\n",
        "          loss_fn: nn.Module,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          n_epochs: int, device: str):\n",
        "    \"\"\"\n",
        "    Trains the inputted model using the provided data, optimizer, loss\n",
        "    function, and device for n epochs. Prints the average loss of each epoch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        model to train\n",
        "    train_dataloader : DataLoader\n",
        "        training data\n",
        "    loss_fn : nn.Module\n",
        "        loss function to use with training\n",
        "    optimizer : torch.optim.Optimizer\n",
        "        optimizer to use with training\n",
        "    n_epochs : int\n",
        "        number of epochs to train for\n",
        "    device : str\n",
        "        device that the model is running on\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    nn.Module\n",
        "        trained model\n",
        "    \"\"\"\n",
        "\n",
        "    model = model.to(device)\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        avg_loss = train_epoch(model, train_dataloader, loss_fn, optimizer, device)\n",
        "        print(\"Epoch:\", epoch, \"Loss:\", avg_loss)\n",
        "    return model"
      ],
      "metadata": {
        "id": "UF0LsdqIdaJS"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "nMrb0t96jwg5"
      },
      "outputs": [],
      "source": [
        "def evaluate(model: nn.Module, dataloader: DataLoader, device: str=\"cuda\") -> tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Evaluates your model!\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        your model!\n",
        "    dataloader : DataLoader\n",
        "        a dataloader of the testing data from build_dataloaders\n",
        "    device : str\n",
        "        device that the model is running on\n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    tuple[int, int]\n",
        "        per-token accuracy and exact_match accuracy\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    tot_tokens = 0\n",
        "    tot_correct = 0\n",
        "    tot_exact_matches = 0\n",
        "    tot_sequences = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input, tgt in dataloader:\n",
        "            input, tgt = input.to(device).permute(1, 0), tgt.to(device).permute(1, 0)\n",
        "            outputs = model(input, tgt)\n",
        "            greedy_predict = outputs.argmax(2)\n",
        "\n",
        "            end_idxs = []\n",
        "            for i in range(tgt.size(0)):\n",
        "                seq = tgt[i, :]\n",
        "                end_idx = torch.where(seq == LF_EOS_INDEX)[0]\n",
        "                end_idxs.append(end_idx.item())\n",
        "                break\n",
        "            end_idxs = torch.tensor(end_idxs)\n",
        "\n",
        "            for batch_idx in range(tgt.size(0)):\n",
        "                model_comp = greedy_predict[batch_idx, :end_idxs[batch_idx] + 1]\n",
        "                tgt_comp = tgt[batch_idx, :end_idxs[batch_idx] + 1]\n",
        "                fully_correct = True\n",
        "                for i in range(tgt_comp.size(0)):\n",
        "                    if(model_comp[i] == tgt_comp[i]):\n",
        "                        tot_correct += 1\n",
        "                    else:\n",
        "                        fully_correct = False\n",
        "                    tot_tokens += 1\n",
        "                if(fully_correct):\n",
        "                    tot_exact_matches += 1\n",
        "                tot_sequences += 1\n",
        "    per_token_accuracy = tot_correct / tot_tokens\n",
        "    exact_match_accurancy = tot_exact_matches / tot_sequences\n",
        "    return per_token_accuracy, exact_match_accurancy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOkicC3yLkfv"
      },
      "source": [
        "# Run this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "0qSnLCPeiI1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aefa6a43-439f-45ee-bfa3-75ce006fef1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Loss: 0.8169908492515484\n",
            "Epoch: 2 Loss: 0.24065837015708288\n",
            "Epoch: 3 Loss: 0.18033159400026003\n",
            "Epoch: 4 Loss: 0.12182168817768495\n",
            "Epoch: 5 Loss: 0.08884651617457469\n",
            "Test Per-token Accuracy: 0.9392265193370166\n",
            "Test Exact-match Accuracy: 0.5357142857142857\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    jobs_train, jobs_test = build_datasets()\n",
        "    dataloader_train, dataloader_test = build_dataloaders(jobs_train, jobs_test, train_batch_size=21)\n",
        "    model = create_model()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.003)\n",
        "    loss_fn = nn.NLLLoss()\n",
        "    model = train(model, dataloader_train, loss_fn, optimizer, n_epochs=5, device=device)\n",
        "    test_per_token_accuracy, test_exact_match_accuracy = evaluate(model, dataloader_test, device=device)\n",
        "    print(f'Test Per-token Accuracy: {test_per_token_accuracy}')\n",
        "    print(f'Test Exact-match Accuracy: {test_exact_match_accuracy}')\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EyRV7swLaglq"
      },
      "execution_count": 32,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}